{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cyKedJ_myMqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "GtpIunp6-WDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXxE47WLR-s0"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import nltk\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "stopwords.words(\"english\")[:10] # <-- import the english stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Data"
      ],
      "metadata": {
        "id": "oeUkOhzrZBBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = pd.read_csv('/content/drive/MyDrive/Amazon_Product_data/dataset.csv')\n",
        "raw_data.head()"
      ],
      "metadata": {
        "id": "wV13DCSiY_AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data.shape"
      ],
      "metadata": {
        "id": "LehOrRrdK1Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using NLTK Pre-trained Sentiment Analyzer for getting the sentiment for the news headline.\n"
      ],
      "metadata": {
        "id": "EFJBMsyEfxXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count = 0\n",
        "for i,row in raw_data.iterrows():\n",
        "  count+=1\n",
        "  text = row['Headline']\n",
        "  sia_object = SIA()\n",
        "  sentiment_scores = sia_object.polarity_scores(text = text)\n",
        "  print(sentiment_scores)\n",
        "  max_sentiment = max(sentiment_scores, key= lambda x: sentiment_scores[x])\n",
        "  # raw_data['sentiment']=max_sentiment\n",
        "  print(max_sentiment)\n",
        "print(count)\n"
      ],
      "metadata": {
        "id": "L7EqJEv_ZKyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After doing nltk sentiment intensity analyzer ,we got only neutral sentiment over 50 news headlines.So we are not doing sentiment  prediction with supervised approach. We are focusing on the unsupervised approach."
      ],
      "metadata": {
        "id": "aSs57Kvukhv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering"
      ],
      "metadata": {
        "id": "ikHMYIo76uN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Pre-Processing"
      ],
      "metadata": {
        "id": "wt2j_ifDk8zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data.isnull().sum()"
      ],
      "metadata": {
        "id": "UiyQXV3dalcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
        "    \"\"\"This utility function sanitizes a string by:\n",
        "    - removing links\n",
        "    - removing special characters\n",
        "    - removing numbers\n",
        "    - removing stopwords\n",
        "    - transforming in lowercase\n",
        "    - removing excessive whitespaces\n",
        "    Args:\n",
        "        text (str): the input text you want to clean\n",
        "        remove_stopwords (bool): whether or not to remove stopwords\n",
        "    Returns:\n",
        "        str: the cleaned text\n",
        "    \"\"\"\n",
        "\n",
        "    # remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # remove special chars and numbers\n",
        "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
        "    # remove stopwords\n",
        "    if remove_stopwords:\n",
        "        # 1. tokenize\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        # 2. check if stopword\n",
        "        tokens = [w for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
        "        # 3. join back together\n",
        "        text = \" \".join(tokens)\n",
        "    # return text in lower case and stripped of whitespaces\n",
        "    text = text.lower().strip()\n",
        "    # print(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "hF2ehyrcmW9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = raw_data.copy()"
      ],
      "metadata": {
        "id": "O2OmATveoOhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = raw_data.copy()"
      ],
      "metadata": {
        "id": "LIwn_j-woD5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.head()"
      ],
      "metadata": {
        "id": "LGBYIs_doSlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.shape"
      ],
      "metadata": {
        "id": "0CoW3RFeEVmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "id": "cuu_6MhyK8qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tf-Idf Vectorization"
      ],
      "metadata": {
        "id": "FggN2Qarobwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(df):\n",
        "  # initialize the vectorizer\n",
        "  vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
        "  # fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
        "  X = vectorizer.fit_transform(df['cleaned_head_line'])\n",
        "  print(X.shape)\n",
        "  return X,vectorizer"
      ],
      "metadata": {
        "id": "1N1ErTj9oWtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PCA"
      ],
      "metadata": {
        "id": "gCJ65Bq091xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_pca(feature_vector):\n",
        "    # initialize PCA with 2 components\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    # pass our X to the pca and store the reduced vectors into pca_vecs\n",
        "    pca_vecs = pca.fit_transform(feature_vector.toarray())\n",
        "    # save our two dimensions into x0 and x1\n",
        "    x0 = pca_vecs[:, 0]\n",
        "    x1 = pca_vecs[:, 1]\n",
        "    return x0,x1"
      ],
      "metadata": {
        "id": "-ZtMC7n0443S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elbow Method"
      ],
      "metadata": {
        "id": "1ckZOBof9y67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def elbow_method(pca_vector,method ='Kmeans'):\n",
        "    \"\"\"\n",
        "    This is the function used to get optimal number of clusters in order to feed to the k-means clustering algorithm.\n",
        "    \"\"\"\n",
        "    if method =='Kmeans':\n",
        "\n",
        "      number_clusters = range(1, 7)  # Range of possible clusters that can be generated\n",
        "      kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters] # Getting no. of clusters \n",
        "\n",
        "      score = [kmeans[i].fit(pca_vector).score(pca_vector) for i in range(len(kmeans))] # Getting score corresponding to each cluster.\n",
        "      score = [i*-1 for i in score] # Getting list of positive scores.\n",
        "      \n",
        "      plt.plot(number_clusters, score)\n",
        "      plt.xlabel('Number of Clusters')\n",
        "      plt.ylabel('Score')\n",
        "      plt.title('Elbow Method')\n",
        "      plt.show()\n",
        "    else:\n",
        "      number_clusters = range(1, 7)  # Range of possible clusters that can be generated\n",
        "      kmeans = [MiniBatchKMeans(n_clusters=i, max_iter = 600) for i in number_clusters] # Getting no. of clusters \n",
        "\n",
        "      score = [kmeans[i].fit(pca_vector).score(pca_vector) for i in range(len(kmeans))] # Getting score corresponding to each cluster.\n",
        "      score = [i*-1 for i in score] # Getting list of positive scores.\n",
        "      \n",
        "      plt.plot(number_clusters, score)\n",
        "      plt.xlabel('Number of Clusters')\n",
        "      plt.ylabel('Score')\n",
        "      plt.title('Elbow Method')\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "pK-vXrfs6iJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Keyword Extraction"
      ],
      "metadata": {
        "id": "Qqplb0um9xYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_keywords(clusters,n_terms,feature_vector,vectorizer):\n",
        "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
        "    df = pd.DataFrame(feature_vector.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
        "    terms = vectorizer.get_feature_names_out() # access tf-idf terms\n",
        "    for i,r in df.iterrows():\n",
        "        print('\\nCluster {}'.format(i))\n",
        "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
        "            "
      ],
      "metadata": {
        "id": "IipmeHhc9qeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_pca(df):\n",
        "    # set image size\n",
        "  plt.figure(figsize=(12, 7))\n",
        "  # set a title\n",
        "  plt.title(\"TF-IDF + KMeans clustering\", fontdict={\"fontsize\": 18})\n",
        "  # set axes names\n",
        "  plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
        "  plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
        "  # create scatter plot with seaborn, where hue is the class used to group the data\n",
        "  sns.scatterplot(data=df, x='x0', y='x1', hue='Cluster', palette=\"viridis\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "RNRHHP19_ERd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_wordcloud(text):\n",
        "\n",
        "  print(text)\n",
        "  # wordcloud = WordCloud().generate(text)\n",
        "  wordcloud = WordCloud(width = 2000, height = 2000,\n",
        "                  background_color ='white',\n",
        "                  min_font_size = 10).generate(text)\n",
        "\n",
        "  # plot the WordCloud image                      \n",
        "  plt.figure(figsize = (8, 8), facecolor = None)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  \n",
        "  # plt.show()\n",
        "  # plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  # plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "nfHuPpbc_sPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means Clustering"
      ],
      "metadata": {
        "id": "pZuZlLTZpC1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Kmeans_Clustering(df,method='Kmeans'):\n",
        "\n",
        "  df['cleaned_head_line'] = df['Headline'].apply(lambda x: preprocess_text(x, remove_stopwords=True))\n",
        "  feature_vector,vectorizer = tf_idf(df)\n",
        "  x0,x1 = find_pca(feature_vector)\n",
        "  df['x0'] = x0\n",
        "  df['x1'] = x1\n",
        "  elbow_method(feature_vector,method = method)\n",
        "  # initialize kmeans with 3 centroids\n",
        "  kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "  # fit the model\n",
        "  kmeans.fit(feature_vector)\n",
        "  # store cluster labels in a variable\n",
        "  clusters = kmeans.labels_\n",
        "  print([c for c in clusters][:10])\n",
        "  df['Cluster'] = clusters\n",
        "  print(Counter(kmeans.labels_))\n",
        "  \n",
        "  get_top_keywords(clusters,10,feature_vector,vectorizer)\n",
        "  # map clusters to appropriate labels \n",
        "  cluster_map = {0: \"Neutral\", 1: \"Positive\", 2: \"Negative\"}\n",
        "  # apply mapping\n",
        "  df['Cluster'] = df['Cluster'].map(cluster_map)\n",
        "  print(df.Cluster.value_counts())\n",
        "  #Positive Word Cloud\n",
        "  df_positive = df[df[\"Cluster\"] == 'Positive']\n",
        "  text_pos = df_positive.cleaned_head_line.str.cat()\n",
        "  #Positive Word Cloud\n",
        "  df_negative = df[df[\"Cluster\"] == 'Negative']\n",
        "  text_neg = df_negative.cleaned_head_line.str.cat()\n",
        "  #Positive Word Cloud\n",
        "  df_neutral = df[df[\"Cluster\"] == 'Neutral']\n",
        "  text_neu = df_neutral.cleaned_head_line.str.cat()\n",
        "\n",
        "  print(df_positive.head())\n",
        "  print(df_positive.shape)\n",
        "  \n",
        "  show_pca(df)\n",
        "  show_wordcloud(text_pos)\n",
        "  show_wordcloud(text_neg)\n",
        "  show_wordcloud(text_neu)\n",
        "  df.to_csv('Kmeans_Cluster_result.csv',index=False)\n",
        "  return df"
      ],
      "metadata": {
        "id": "M9vPogptS4zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BagOfWords"
      ],
      "metadata": {
        "id": "SVaLtQ8uRsKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BOW\n",
        "def bow_features(df):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer()\n",
        "    x = vectorizer.fit_transform(df['cleaned_head_line'].tolist())            \n",
        "    bow = x.toarray()   \n",
        "    print(x.shape)\n",
        "    \n",
        "    return x,vectorizer"
      ],
      "metadata": {
        "id": "BNBhwOkTyzPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MiniBatchKmeans"
      ],
      "metadata": {
        "id": "B8BGkkoAB-a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MBK_Clustering(df,method='MBK'):\n",
        "  df['cleaned_head_line'] = df['Headline'].apply(lambda x: preprocess_text(x, remove_stopwords=True))\n",
        "  feature_vector,vectorizer = bow_features(df)\n",
        "  x0,x1 = find_pca(feature_vector)\n",
        "  df['x0'] = x0\n",
        "  df['x1'] = x1\n",
        "  elbow_method(feature_vector,method = method)\n",
        "  # initialize kmeans with 3 centroids\n",
        "  mbk = MiniBatchKMeans(init ='k-means++', n_clusters = 3,\n",
        "                      batch_size = 10, n_init = 10,\n",
        "                      max_no_improvement = 10, verbose = 0)\n",
        "\n",
        "  mbk.fit(feature_vector)\n",
        "  clusters = mbk.labels_\n",
        "  print([c for c in clusters][:10])\n",
        "  df['Cluster'] = clusters\n",
        "  print(Counter(mbk.labels_))\n",
        "  \n",
        "  get_top_keywords(clusters,10,feature_vector,vectorizer)\n",
        "  # map clusters to appropriate labels \n",
        "  cluster_map = {0: \"Neutral\", 1: \"Positive\", 2: \"Negative\"}\n",
        "  # apply mapping\n",
        "  df['Cluster'] = df['Cluster'].map(cluster_map)\n",
        "  print(df.Cluster.value_counts())\n",
        "  #Positive Word Cloud\n",
        "  df_positive = df[df[\"Cluster\"] == 'Positive']\n",
        "  text_pos = df_positive.cleaned_head_line.str.cat()\n",
        "  #Positive Word Cloud\n",
        "  df_negative = df[df[\"Cluster\"] == 'Negative']\n",
        "  text_neg = df_negative.cleaned_head_line.str.cat()\n",
        "  #Positive Word Cloud\n",
        "  df_neutral = df[df[\"Cluster\"] == 'Neutral']\n",
        "  text_neu = df_neutral.cleaned_head_line.str.cat()\n",
        "\n",
        "  print(df_positive.head())\n",
        "  print(df_positive.shape)\n",
        "  \n",
        "  show_pca(df)\n",
        "  show_wordcloud(text_pos)\n",
        "  show_wordcloud(text_neg)\n",
        "  show_wordcloud(text_neu)\n",
        "  df.to_csv('MBK_Cluster_result.csv',index=False)\n",
        "  return df"
      ],
      "metadata": {
        "id": "ICSi2FSJ__Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.head()"
      ],
      "metadata": {
        "id": "SCm9BsWqDjag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.shape"
      ],
      "metadata": {
        "id": "kX9uJzmqE8b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kmeans_df = Kmeans_Clustering(data1)\n"
      ],
      "metadata": {
        "id": "ZDFm5nzfCIQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kmeans_df.head()"
      ],
      "metadata": {
        "id": "ImENxsnSDRf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "id": "JQ79QMsZJkrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MBK_df = MBK_Clustering(data2)\n"
      ],
      "metadata": {
        "id": "Q8CjD9Q_Cuzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MBK_df.head()"
      ],
      "metadata": {
        "id": "lVwx9yZMC9V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MBK_df.to_csv('MBK_Cluster_result.csv',index=False)"
      ],
      "metadata": {
        "id": "E-6GYztQ0UCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we found the sentiment for our headlines with Kmeans and Mini batch Kmeans clustering methods. We got different results from two methods and the cluster forms also different. We got more balanced result from Kmeans algorithm rather than MiniBatch Kmeans.\n",
        "\n",
        "Then  we are doing the classification with our clustered data using Naivebayes classification algorithm."
      ],
      "metadata": {
        "id": "cqCUb-sLLe5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "LbjghFH86x1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Kmeans clusters for classification.We are training Naive Bayes algorithm for predicting the sentiments. Because in Kmeans , we have more balanced data than Mini Batch Kmeans."
      ],
      "metadata": {
        "id": "ADSEJWCvyJgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "m_jD0AQjyVKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loading Data"
      ],
      "metadata": {
        "id": "OgUJDVck04oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report,accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "plt.xticks(rotation=70)\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "MaFzpbM1JQuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Amazon_Product_data/Kmeans_Cluster_result.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "YDXAkJFt6ctd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "V03nTs181SSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = data[['Description','Cluster']].copy()\n",
        "new_data.rename(columns={'Cluster': 'Sentiment'}, inplace=True)"
      ],
      "metadata": {
        "id": "xLVQs4Ux1YM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.head()"
      ],
      "metadata": {
        "id": "ws2X1xOd9KzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
        "    \"\"\"This utility function sanitizes a string by:\n",
        "    - removing links\n",
        "    - removing special characters\n",
        "    - removing numbers\n",
        "    - removing stopwords\n",
        "    - transforming in lowercase\n",
        "    - removing excessive whitespaces\n",
        "    Args:\n",
        "        text (str): the input text you want to clean\n",
        "        remove_stopwords (bool): whether or not to remove stopwords\n",
        "    Returns:\n",
        "        str: the cleaned text\n",
        "    \"\"\"\n",
        "    lm=WordNetLemmatizer()\n",
        "    # remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # remove special chars and numbers\n",
        "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
        "    # remove stopwords\n",
        "    if remove_stopwords:\n",
        "        # 1. tokenize\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        # 2. check if stopword\n",
        "        tokens = [w for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
        "        \n",
        "        lemmed_words = [lm.lemmatize(word) for word in tokens]\n",
        "        # cleaned_word_list = ' '.join(lemmed_words)\n",
        "        # 3. join back together\n",
        "        text = \" \".join(lemmed_words)\n",
        "    # return text in lower case and stripped of whitespaces\n",
        "    text = text.lower().strip()\n",
        "    # print(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "_V6K17PWAtRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['cleaned_description']= new_data['Description'].apply(lambda x: preprocess_text(x, remove_stopwords=True))\n"
      ],
      "metadata": {
        "id": "uPrSUYdy9MpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.head()"
      ],
      "metadata": {
        "id": "vkODy1P2ArDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['text_blob_sentiment'] = new_data['cleaned_description'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "new_data.head()"
      ],
      "metadata": {
        "id": "iBlIvHmoR2Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(50,30))\n",
        "plt.margins(0.02)\n",
        "plt.xlabel('text_blob_sentiment', fontsize=50)\n",
        "plt.xticks(fontsize=40)\n",
        "plt.ylabel('Frequency', fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.hist(new_data['text_blob_sentiment'], bins=50)\n",
        "plt.title('text_blob_sentiment Distribution', fontsize=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Osu3hghLTaqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['word_count'] = new_data['cleaned_description'].apply(lambda x: len(str(x).split()))\n",
        "new_data['text_len'] = new_data['cleaned_description'].astype(str).apply(len)\n",
        "new_data.head()"
      ],
      "metadata": {
        "id": "DMj0PyGbTrAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.describe()"
      ],
      "metadata": {
        "id": "RoiNsfLUh9Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation = new_data[['text_blob_sentiment', 'text_len', 'word_count']].corr()\n",
        "mask = np.zeros_like(correlation, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.xticks(fontsize=40)\n",
        "plt.yticks(fontsize=40)\n",
        "sns.heatmap(correlation, cmap='coolwarm', annot=True, annot_kws={\"size\": 40}, linewidths=10, vmin=-1.5, mask=mask)"
      ],
      "metadata": {
        "id": "NyCnnl1xUvMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_wordcloud(text):\n",
        "\n",
        "  print(text)\n",
        "  # wordcloud = WordCloud().generate(text)\n",
        "  wordcloud = WordCloud(width = 2000, height = 2000,\n",
        "                  background_color ='white',\n",
        "                  min_font_size = 10).generate(text)\n",
        "\n",
        "  # plot the WordCloud image                      \n",
        "  plt.figure(figsize = (8, 8), facecolor = None)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  \n",
        "  # plt.show()\n",
        "  # plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  # plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "3oFFvmWbZZqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words =  new_data.cleaned_description.str.cat()\n",
        "print(len(words))\n",
        "show_wordcloud(words)"
      ],
      "metadata": {
        "id": "7yp7BVeaVz6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing The Sentiment Distribution"
      ],
      "metadata": {
        "id": "30kuLmBD3znN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Countplot\n",
        "sns.countplot(new_data.Sentiment)"
      ],
      "metadata": {
        "id": "5n-fBd063rFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = new_data['cleaned_description'].values.tolist()\n",
        "allwords = []\n",
        "for doc in words:\n",
        "    \n",
        "    allwords.extend(doc.split())\n",
        "print(allwords)\n",
        "print(len(allwords))\n",
        "# show_wordcloud(words)"
      ],
      "metadata": {
        "id": "rWYwz5tecxP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostcommon = FreqDist(allwords).most_common(100)\n",
        "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(mostcommon))\n",
        "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('Top 100 Most Common Words', fontsize=100)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lCSPDEgUWTaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostcommon_small = FreqDist(allwords).most_common(25)\n",
        "x, y = zip(*mostcommon_small)\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.margins(0.02)\n",
        "plt.bar(x, y)\n",
        "plt.xlabel('Words', fontsize=50)\n",
        "plt.ylabel('Frequency of Words', fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60, fontsize=40)\n",
        "plt.title('Frequency of 25 Most Common Words', fontsize=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BXTPCluMXOWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Train Data"
      ],
      "metadata": {
        "id": "niGn-NPXipMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.head()"
      ],
      "metadata": {
        "id": "d9s_XIT3iDQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.to_csv('classification_data.csv',index=False)"
      ],
      "metadata": {
        "id": "jBJyTHjZzBcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = new_data[['cleaned_description','Sentiment']].copy()\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "lVqHNjwiiP6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Label Encoding the Target Sentiment Column\n"
      ],
      "metadata": {
        "id": "x2ZhMrUajGQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "  \n",
        "# Encode labels in column 'species'.\n",
        "train_data['Sentiment']= label_encoder.fit_transform(train_data['Sentiment'])\n",
        "  \n",
        "train_data['Sentiment'].unique()\n",
        "train_data[\"Sentiment\"] = train_data[\"Sentiment\"].astype('category')"
      ],
      "metadata": {
        "id": "m_dcUzGojE3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtypes"
      ],
      "metadata": {
        "id": "Gd9q4P1Cl-C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "list(label_encoder.inverse_transform([0,1,2]))"
      ],
      "metadata": {
        "id": "KOYKJrvLkWbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "gLNyg5sOmPPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=train_data['cleaned_description']\n",
        "y = train_data['Sentiment']\n",
        "X.head()"
      ],
      "metadata": {
        "id": "yOYF11zjn-SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X)"
      ],
      "metadata": {
        "id": "yUDxkqeKqSuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(y)"
      ],
      "metadata": {
        "id": "5lLC7WmYoZIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1, max_df=0.5)\n",
        "def tf_idf(X):\n",
        "  # initialize the vectorizer\n",
        "  \n",
        "  # fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
        "  X = vectorizer.fit_transform(X).toarray()\n",
        "  print(X.shape)\n",
        "  return X"
      ],
      "metadata": {
        "id": "iYHyVa4UpzcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= tf_idf(X)"
      ],
      "metadata": {
        "id": "w92sHPdwp3Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test, Y_train, Y_test = train_test_split(X, y,test_size=0.10,random_state = 5,stratify = train_data['Sentiment'])"
      ],
      "metadata": {
        "id": "MYihN3uGwGvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multinomial Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "WClKZWp5tcMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "v4eYFzjpYNcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "eTvrNOkYtjy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "tDrXT-w9udg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "id": "UIutm8szwtYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Actual value and the predicted value\n",
        "pred_df = pd.DataFrame({'Actual value': Y_test, 'Predicted value':y_pred})\n",
        "print(model.get_params(deep=True))\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "92Sp8TXjxIKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "1jj4phM0xgBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Data"
      ],
      "metadata": {
        "id": "I66z0zIfvRQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(Y_test, y_pred,labels=model.classes_)\n",
        " \n",
        "print (\"Confusion Matrix : \\n\", cm)\n",
        "disp = ConfusionMatrixDisplay.from_predictions(Y_test,y_pred)\n",
        "# disp.plot()\n",
        "plt.savefig('MNB_CM_test_data.png')\n",
        "plt.show()\n",
        "print(cm.ravel())\n",
        "# tn, fp, fn, tp = cm.ravel()\n",
        "# print(tn, fp, fn, tp)\n",
        "\n",
        "#Accuracy\n",
        "print (\"Accuracy : \", accuracy_score(Y_test, y_pred))\n",
        "\n",
        "#Precision Recall and F1-score\n",
        "# logit_model=sm.Logit(y,X)\n",
        "# result=logit_model.fit()\n",
        "# print(result.summary2())\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "dCmlkD6DxVhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Data"
      ],
      "metadata": {
        "id": "fm8Nm7ZqvTJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_train)"
      ],
      "metadata": {
        "id": "prPie7UYwl7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Actual value and the predicted value\n",
        "pred_df = pd.DataFrame({'Actual value': Y_train, 'Predicted value':y_pred})\n",
        "print(model.get_params(deep=True))\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "Ezy4UUdVwtev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(Y_train, y_pred,labels=model.classes_)\n",
        " \n",
        "print (\"Confusion Matrix : \\n\", cm)\n",
        "disp = ConfusionMatrixDisplay.from_predictions(Y_train,y_pred)\n",
        "# disp.plot()\n",
        "plt.savefig('MNB_CM_Train_data.png')\n",
        "plt.show()\n",
        "print(cm.ravel())\n",
        "# tn, fp, fn, tp = cm.ravel()\n",
        "# print(tn, fp, fn, tp)\n",
        "\n",
        "#Accuracy\n",
        "print (\"Accuracy : \", accuracy_score(Y_train, y_pred))\n",
        "\n",
        "#Precision Recall and F1-score\n",
        "# logit_model=sm.Logit(y,X)\n",
        "# result=logit_model.fit()\n",
        "# print(result.summary2())\n",
        "print(classification_report(Y_train, y_pred))"
      ],
      "metadata": {
        "id": "kBwbeoaOvGL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "Here, we tried first clustering and then classification for sentiment analysis. But the results from both methods are not satisafctory due to the less amount of records. We have  done exploratory data analysis with clustered data and then applied naive bayes algorithm\n"
      ],
      "metadata": {
        "id": "IRezFT92xU7X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7263Dx3w-0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}